"""
boosts_gridsearch_150k_progress.py
GridSearchCV Ğ´Ğ»Ñ LightGBM Ğ¸ XGBoost
150k ÑÑ‚Ñ€Ğ¾Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚!
Ğ¡ ĞŸĞĞ”Ğ ĞĞ‘ĞĞ«Ğœ ĞŸĞ ĞĞ“Ğ Ğ•Ğ¡Ğ¡ĞĞœ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸!
"""

import pandas as pd
import numpy as np
import time
import json
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯ Ğ”Ğ›Ğ¯ Ğ¡ĞĞ¥Ğ ĞĞĞ•ĞĞ˜Ğ¯ Ğ˜ Ğ¡ĞšĞĞ§Ğ˜Ğ’ĞĞĞ˜Ğ¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_and_download(data, filename):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² JSON Ğ¸ ÑÑ€Ğ°Ğ·Ñƒ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2)
    
    try:
        from google.colab import files
        files.download(filename)
        print(f"   ğŸ“¥ {filename} ÑĞºĞ°Ñ‡Ğ°Ğ½!")
    except:
        print(f"   ğŸ’¾ {filename} ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾")

print("=" * 80)
print("ğŸš€ GRIDSEARCHCV Ğ”Ğ›Ğ¯ Ğ‘Ğ£Ğ¡Ğ¢Ğ˜ĞĞ“ĞĞ’ â€” 150K Ğ¡Ğ¢Ğ ĞĞš")
print("=" * 80)
print(f"ğŸ• ĞĞ°Ñ‡Ğ°Ğ»Ğ¾: {time.strftime('%H:%M:%S')}")

total_start = time.time()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.read_csv('train_fixed.csv')
print(f"\nâœ… Ğ’ÑĞµĞ³Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {len(df):,} ÑÑ‚Ñ€Ğ¾Ğº")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ ĞĞ—Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ•: 150k train, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ test
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TRAIN_SIZE = 150000
TEST_SIZE = 100000

df_train_full, df_test = train_test_split(
    df, 
    train_size=TRAIN_SIZE + TEST_SIZE, 
    stratify=df['Heart Disease'],
    random_state=42
)

df_train, _ = train_test_split(
    df_train_full,
    train_size=TRAIN_SIZE,
    stratify=df_train_full['Heart Disease'],
    random_state=42
)

print(f"âœ… Train: {len(df_train):,} ÑÑ‚Ñ€Ğ¾Ğº")
print(f"âœ… Test: {len(df_test):,} ÑÑ‚Ñ€Ğ¾Ğº")

# ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ°
X_train = df_train.drop('Heart Disease', axis=1)
y_train = df_train['Heart Disease'].map({'Absence': 0, 'Presence': 1})

X_test = df_test.drop('Heart Disease', axis=1)
y_test = df_test['Heart Disease'].map({'Absence': 0, 'Presence': 1})

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("=" * 80)

# Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ (Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 630k)
baseline = {
    'LightGBM': 0.8725799189554255,
    'XGBoost': 0.8720027017899359,
    'VotingEnsemble': 0.869309838472834
}

all_results = []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯ Ğ”Ğ›Ğ¯ Ğ’Ğ«Ğ’ĞĞ”Ğ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢ĞĞ’ GRIDSEARCH
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def print_gridsearch_results(grid_result, model_name, start_time, param_grid):
    """Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ GridSearch"""
    elapsed = time.time() - start_time
    
    # Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸
    total_combos = 1
    for key, values in param_grid.items():
        total_combos *= len(values)
    total_fits = total_combos * 5
    
    print(f"\nğŸ“Š ĞŸĞ ĞĞ“Ğ Ğ•Ğ¡Ğ¡ {model_name}:")
    print(f"   Ğ’ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: {total_combos}")
    print(f"   Ğ’ÑĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¹ (5 Ñ„Ğ¾Ğ»Ğ´Ğ¾Ğ²): {total_fits}")
    print(f"   ĞŸÑ€Ğ¾ÑˆĞ»Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸: {elapsed/60:.2f} Ğ¼Ğ¸Ğ½")
    print(f"   Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: {elapsed/total_fits:.2f} ÑĞµĞº")
    
    # Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
    print(f"\nğŸ“‹ Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ«Ğ• Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« ({model_name}):")
    print(f"{'Ğ Ğ°Ğ½Ğ³':<6} {'F1 (cv)':<12} {'F1 (test)':<12} {'ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹':<60}")
    print("-" * 100)
    
    cv_results = pd.DataFrame(grid_result.cv_results_)
    cv_results = cv_results.sort_values('rank_test_f1')
    
    for idx, row in cv_results.iterrows():
        params = row['params']
        param_str = str(params).replace('{', '').replace('}', '').replace("'", '')
        print(f"{int(row['rank_test_f1']):<6} {row['mean_test_score']:<12.4f} {row['rank_test_f1']:<12} {param_str:<60}")
    
    return elapsed

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞœĞĞ”Ğ•Ğ›Ğ¬ 1: LightGBM â€” GridSearchCV
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("1ï¸âƒ£ LightGBM â€” GridSearchCV")
print("=" * 80)

start = time.time()

param_grid_lgb = {
    'n_estimators': [150, 200, 250],
    'max_depth': [6, 8, 10],
    'learning_rate': [0.05, 0.1, 0.15],
    'num_leaves': [31, 45, 63],
    'min_child_samples': [15, 20, 25],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# ĞŸĞ¾Ğ´ÑÑ‡Ñ‘Ñ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹
total_lgb = 1
for key, values in param_grid_lgb.items():
    total_lgb *= len(values)
print(f"\nğŸ“Š Ğ’Ğ¡Ğ•Ğ“Ğ ĞšĞĞœĞ‘Ğ˜ĞĞĞ¦Ğ˜Ğ™: {total_lgb} Ã— 5 Ñ„Ğ¾Ğ»Ğ´Ğ¾Ğ² = {total_lgb * 5} Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¹")
print(f"ğŸ“‹ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:")
for key, values in param_grid_lgb.items():
    print(f"   {key}: {values}")
print("=" * 80)

from lightgbm import LGBMClassifier

grid_lgb = GridSearchCV(
    LGBMClassifier(random_state=42, verbose=-1, n_jobs=-1),
    param_grid_lgb, cv=5, scoring='f1', n_jobs=-1, verbose=2
)
grid_lgb.fit(X_train_scaled, y_train)

elapsed = time.time() - start

# Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°
print_gridsearch_results(grid_lgb, 'LightGBM', start, param_grid_lgb)

y_pred = grid_lgb.predict(X_test_scaled)
y_proba = grid_lgb.predict_proba(X_test_scaled)[:, 1]

results_lgb = {
    'model': 'LightGBM_GridSearch_150k',
    'time_min': elapsed / 60,
    'best_params': grid_lgb.best_params_,
    'cv_best_score': float(grid_lgb.best_score_),
    'test_accuracy': float(accuracy_score(y_test, y_pred)),
    'test_f1': float(f1_score(y_test, y_pred)),
    'test_roc_auc': float(roc_auc_score(y_test, y_proba)),
    'baseline_f1': baseline['LightGBM'],
    'improvement': float(f1_score(y_test, y_pred)) - baseline['LightGBM'],
    'train_size': len(df_train),
    'test_size': len(df_test)
}

save_and_download(results_lgb, 'results_lightgbm_150k_grid.json')
all_results.append(results_lgb)

print(f"\nâ±ï¸ {elapsed/60:.2f} Ğ¼Ğ¸Ğ½ | F1: {results_lgb['test_f1']:.4f}")
print(f"ğŸ“‹ Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: {grid_lgb.best_params_}")
print(f"ğŸ“ˆ Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ñ‹ (630k): {results_lgb['improvement']:+.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞœĞĞ”Ğ•Ğ›Ğ¬ 2: XGBoost â€” GridSearchCV
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("2ï¸âƒ£ XGBoost â€” GridSearchCV")
print("=" * 80)

start = time.time()

param_grid_xgb = {
    'n_estimators': [150, 200, 250],
    'max_depth': [5, 7, 9],
    'learning_rate': [0.05, 0.1, 0.15],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'gamma': [0, 0.1, 0.2],
    'min_child_weight': [1, 3, 5]
}

# ĞŸĞ¾Ğ´ÑÑ‡Ñ‘Ñ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹
total_xgb = 1
for key, values in param_grid_xgb.items():
    total_xgb *= len(values)
print(f"\nğŸ“Š Ğ’Ğ¡Ğ•Ğ“Ğ ĞšĞĞœĞ‘Ğ˜ĞĞĞ¦Ğ˜Ğ™: {total_xgb} Ã— 5 Ñ„Ğ¾Ğ»Ğ´Ğ¾Ğ² = {total_xgb * 5} Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¹")
print(f"ğŸ“‹ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:")
for key, values in param_grid_xgb.items():
    print(f"   {key}: {values}")
print("=" * 80)

from xgboost import XGBClassifier

grid_xgb = GridSearchCV(
    XGBClassifier(random_state=42, verbosity=0, n_jobs=-1),
    param_grid_xgb, cv=5, scoring='f1', n_jobs=-1, verbose=2
)
grid_xgb.fit(X_train_scaled, y_train)

elapsed = time.time() - start

# Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°
print_gridsearch_results(grid_xgb, 'XGBoost', start, param_grid_xgb)

y_pred = grid_xgb.predict(X_test_scaled)
y_proba = grid_xgb.predict_proba(X_test_scaled)[:, 1]

results_xgb = {
    'model': 'XGBoost_GridSearch_150k',
    'time_min': elapsed / 60,
    'best_params': grid_xgb.best_params_,
    'cv_best_score': float(grid_xgb.best_score_),
    'test_accuracy': float(accuracy_score(y_test, y_pred)),
    'test_f1': float(f1_score(y_test, y_pred)),
    'test_roc_auc': float(roc_auc_score(y_test, y_proba)),
    'baseline_f1': baseline['XGBoost'],
    'improvement': float(f1_score(y_test, y_pred)) - baseline['XGBoost'],
    'train_size': len(df_train),
    'test_size': len(df_test)
}

save_and_download(results_xgb, 'results_xgboost_150k_grid.json')
all_results.append(results_xgb)

print(f"\nâ±ï¸ {elapsed/60:.2f} Ğ¼Ğ¸Ğ½ | F1: {results_xgb['test_f1']:.4f}")
print(f"ğŸ“‹ Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: {grid_xgb.best_params_}")
print(f"ğŸ“ˆ Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ñ‹ (630k): {results_xgb['improvement']:+.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞœĞĞ”Ğ•Ğ›Ğ¬ 3: VotingEnsemble (LGB + XGB) â€” GridSearch Ğ²ĞµÑĞ¾Ğ²
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("3ï¸âƒ£ VotingEnsemble (LGB + XGB) â€” GridSearch Ğ²ĞµÑĞ¾Ğ²")
print("=" * 80)

start = time.time()

lgb_best = LGBMClassifier(random_state=42, verbose=-1, n_jobs=-1, **grid_lgb.best_params_)
xgb_best = XGBClassifier(random_state=42, verbosity=0, n_jobs=-1, **grid_xgb.best_params_)

param_grid_voting = {
    'weights': [
        [1, 1], [1, 2], [2, 1], [1, 3], [3, 1],
        [2, 3], [3, 2], [1, 4], [4, 1]
    ]
}

total_voting = len(param_grid_voting['weights'])
print(f"\nğŸ“Š Ğ’Ğ¡Ğ•Ğ“Ğ ĞšĞĞœĞ‘Ğ˜ĞĞĞ¦Ğ˜Ğ™: {total_voting} Ã— 5 Ñ„Ğ¾Ğ»Ğ´Ğ¾Ğ² = {total_voting * 5} Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¹")
print(f"ğŸ“‹ Ğ’ĞµÑĞ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸: {param_grid_voting['weights']}")
print("=" * 80)

grid_voting = GridSearchCV(
    VotingClassifier(
        estimators=[('lgb', lgb_best), ('xgb', xgb_best)],
        voting='soft', n_jobs=-1
    ),
    param_grid_voting, cv=5, scoring='f1', n_jobs=-1, verbose=2
)
grid_voting.fit(X_train_scaled, y_train)

elapsed = time.time() - start

# Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°
print_gridsearch_results(grid_voting, 'VotingEnsemble', start, param_grid_voting)

y_pred = grid_voting.predict(X_test_scaled)
y_proba = grid_voting.predict_proba(X_test_scaled)[:, 1]

results_voting = {
    'model': 'VotingEnsemble_Boosts_150k',
    'time_min': elapsed / 60,
    'best_params': grid_voting.best_params_,
    'cv_best_score': float(grid_voting.best_score_),
    'test_accuracy': float(accuracy_score(y_test, y_pred)),
    'test_f1': float(f1_score(y_test, y_pred)),
    'test_roc_auc': float(roc_auc_score(y_test, y_proba)),
    'baseline_f1': baseline['VotingEnsemble'],
    'improvement': float(f1_score(y_test, y_pred)) - baseline['VotingEnsemble'],
    'train_size': len(df_train),
    'test_size': len(df_test)
}

save_and_download(results_voting, 'results_voting_150k_grid.json')
all_results.append(results_voting)

print(f"\nâ±ï¸ {elapsed/60:.2f} Ğ¼Ğ¸Ğ½ | F1: {results_voting['test_f1']:.4f}")
print(f"ğŸ“‹ Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ğ²ĞµÑĞ°: {grid_voting.best_params_}")
print(f"ğŸ“ˆ Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ñ‹ (630k): {results_voting['improvement']:+.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ˜Ğ¢ĞĞ“ĞĞ’Ğ«Ğ™ ĞĞ¢Ğ§ĞĞ¢
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
total_elapsed = time.time() - total_start

print("\n" + "=" * 80)
print("ğŸ“Š Ğ˜Ğ¢ĞĞ“ĞĞ’Ğ«Ğ™ ĞĞ¢Ğ§ĞĞ¢")
print("=" * 80)

print(f"\n{'ĞœĞ¾Ğ´ĞµĞ»ÑŒ':<35} {'Ğ‘Ğ°Ğ·Ğ° (630k)':<12} {'150k Grid':<12} {'Î”':<10} {'Ğ’Ñ€ĞµĞ¼Ñ':<10}")
print("-" * 80)
for res in sorted(all_results, key=lambda x: x['test_f1'], reverse=True):
    print(f"{res['model']:<35} {res['baseline_f1']:<12.4f} {res['test_f1']:<12.4f} {res['improvement']:+.4f} {res['time_min']:<10.2f} Ğ¼Ğ¸Ğ½")

print(f"\nâ±ï¸ ĞĞ‘Ğ©Ğ•Ğ• Ğ’Ğ Ğ•ĞœĞ¯: {total_elapsed/60:.2f} Ğ¼Ğ¸Ğ½ÑƒÑ‚")

# Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
best = max(all_results, key=lambda x: x['test_f1'])
print(f"\nğŸ† Ğ›Ğ£Ğ§Ğ¨ĞĞ¯ ĞœĞĞ”Ğ•Ğ›Ğ¬: {best['model']}")
print(f"   F1-Score:  {best['test_f1']:.4f}")
print(f"   Accuracy:  {best['test_accuracy']:.4f}")
print(f"   ROC-AUC:   {best['test_roc_auc']:.4f}")
print(f"   Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: {best['improvement']:+.4f}")
print(f"   ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: {best['best_params']}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ• Ğ¡ĞšĞĞ§Ğ˜Ğ’ĞĞĞ˜Ğ•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print("ğŸ“¥ Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ• Ğ¡ĞšĞĞ§Ğ˜Ğ’ĞĞĞ˜Ğ•")
print("=" * 80)

summary = {
    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
    'total_time_min': total_elapsed / 60,
    'best_model': best['model'],
    'best_f1': best['test_f1'],
    'train_size': len(df_train),
    'test_size': len(df_test),
    'all_results': all_results
}

save_and_download(summary, 'results_boosts_150k_summary.json')

try:
    import shutil
    shutil.make_archive('results_boosts_150k', 'zip', '.', 'results_')
    save_and_download({}, 'results_boosts_150k.zip')
    print("âœ… ZIP Ğ°Ñ€Ñ…Ğ¸Ğ² ÑĞºĞ°Ñ‡Ğ°Ğ½!")
except Exception as e:
    print(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ZIP: {e}")

print(f"\nğŸ• ĞšĞ¾Ğ½ĞµÑ†: {time.strftime('%H:%M:%S')}")
print("=" * 80)
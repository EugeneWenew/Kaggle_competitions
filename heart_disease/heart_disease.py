import pandas as pd
import numpy as np
import time
import json
import os
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

# ============================================================
# 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# ============================================================

print("=" * 70)
print("üîç –ì–ò–ë–†–ò–î–ù–´–ô –ü–û–ò–°–ö: GridSearch + RandomizedSearch")
print("=" * 70)
print(f"üïê –ù–∞—á–∞–ª–æ: {time.strftime('%H:%M:%S')}")

total_start = time.time()

df = pd.read_csv('train_fixed.csv')
print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df):,}")

X = df.drop('Heart Disease', axis=1)
y = df['Heart Disease'].map({'Absence': 0, 'Presence': 1})

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=10000, random_state=42, stratify=y
)

print(f"üìà Train: {len(X_train):,}, Test: {len(X_test):,}")

# ============================================================
# 2. –ë–ê–ó–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
# ============================================================

baseline_results = {
    'VotingEnsemble': 0.8693,
    'RandomForest': 0.8659,
    'LogisticRegression': 0.8656,
    'LinearSVC': 0.8648
}

# ============================================================
# 3. –ú–û–î–ï–õ–ò –° –£–ñ–ï –ò–ó–í–ï–°–¢–ù–´–ú–ò –†–ï–ó–£–õ–¨–¢–ê–¢–ê–ú–ò (GridSearch)
# ============================================================

gridsearch_models = {
    'LogisticRegression': {
        'model': LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1),
        'params': {
            'C': [0.1, 0.5, 1, 2, 5, 10],
            'class_weight': [None, 'balanced'],
            'solver': ['liblinear', 'saga'],
            'penalty': ['l1', 'l2']
        },
        'baseline_f1': baseline_results.get('LogisticRegression', None)
    },
    
    'RandomForest': {
        'model': RandomForestClassifier(random_state=42, n_jobs=-1),
        'params': {
            'n_estimators': [100, 150, 200, 250],
            'max_depth': [8, 10, 12, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'class_weight': [None, 'balanced']
        },
        'baseline_f1': baseline_results.get('RandomForest', None)
    },
    
    'LinearSVC': {
        'model': LinearSVC(max_iter=1000, random_state=42),
        'params': {
            'C': [0.5, 1, 2, 5, 10],
            'class_weight': [None, 'balanced']
        },
        'baseline_f1': baseline_results.get('LinearSVC', None)
    }
}

# ============================================================
# 4. –ù–û–í–´–ï –ú–û–î–ï–õ–ò (RandomizedSearch)
# ============================================================

randomsearch_models = {
    'XGBoost': {
        'model': XGBClassifier(random_state=42, n_jobs=-1, verbosity=0),
        'params': {
            'n_estimators': [50, 100, 150, 200],
            'max_depth': [3, 5, 7, 9],
            'learning_rate': [0.05, 0.1, 0.2, 0.3],
            'subsample': [0.8, 0.9, 1.0]
        },
        'n_iter': 20
    },
    
    'KNeighbors': {
        'model': KNeighborsClassifier(n_jobs=-1),
        'params': {
            'n_neighbors': [5, 10, 15, 20, 25],
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan', 'minkowski'],
            'p': [1, 2]
        },
        'n_iter': 15
    },
    
    'AdaBoost': {
        'model': AdaBoostClassifier(random_state=42),
        'params': {
            'n_estimators': [50, 100, 150, 200],
            'learning_rate': [0.1, 0.3, 0.5, 0.7, 1.0],
            'algorithm': ['SAMME', 'SAMME.R']
        },
        'n_iter': 15
    }
}

# ============================================================
# 5. –§–£–ù–ö–¶–ò–Ø –°–û–•–†–ê–ù–ï–ù–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
# ============================================================

def save_results_incremental(results, model_name, filename='hybrid_search_results.json'):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                all_results = json.load(f)
        except:
            all_results = []
    else:
        all_results = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    all_results.append(results)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, default=str)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ø–∏—é —Å –∏–º–µ–Ω–µ–º –º–æ–¥–µ–ª–∏
    safe_name = model_name.replace(' ', '_').replace('/', '_')
    individual_file = f'model_result_{safe_name}.json'
    with open(individual_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {individual_file}")
    
    return all_results

# ============================================================
# 6. –û–ë–£–ß–ï–ù–ò–ï: GridSearch –¥–ª—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
# ============================================================

print("\n" + "=" * 70)
print("üìå –≠–¢–ê–ü 1: GridSearch (–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)")
print("=" * 70)

all_results = []
best_models = {}

for name, config in gridsearch_models.items():
    print(f"\n{'=' * 70}")
    print(f"üéØ {name} (GridSearch)")
    print(f"{'=' * 70}")
    
    if config['baseline_f1']:
        print(f"üìä –ë–∞–∑–æ–≤—ã–π F1: {config['baseline_f1']:.4f}")
    
    start = time.time()
    
    grid = GridSearchCV(
        estimator=config['model'],
        param_grid=config['params'],
        cv=5,
        scoring='f1',
        n_jobs=-1,
        verbose=0
    )
    
    grid.fit(X_train, y_train)
    elapsed = time.time() - start
    
    y_pred = grid.predict(X_test)
    y_proba = grid.predict_proba(X_test)[:, 1] if hasattr(grid, 'predict_proba') else None
    
    improvement = (grid.best_score_ - config['baseline_f1']) if config['baseline_f1'] else None
    
    result = {
        'model': name,
        'search_type': 'GridSearch',
        'best_params': grid.best_params_,
        'cv_f1': grid.best_score_,
        'test_f1': f1_score(y_test, y_pred),
        'test_accuracy': accuracy_score(y_test, y_pred),
        'test_roc_auc': roc_auc_score(y_test, y_proba) if y_proba is not None else None,
        'baseline_f1': config['baseline_f1'],
        'improvement': improvement,
        'time_min': elapsed / 60,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    best_models[name] = grid
    all_results = save_results_incremental(result, name)
    
    print(f"‚è±Ô∏è –í—Ä–µ–º—è: {elapsed/60:.1f} –º–∏–Ω")
    print(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid.best_params_}")
    print(f"üìä CV F1: {grid.best_score_:.4f}")
    print(f"üìä Test F1: {f1_score(y_test, y_pred):.4f}")
    if improvement:
        print(f"üìà –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:+.4f}")

# ============================================================
# 7. –û–ë–£–ß–ï–ù–ò–ï: RandomizedSearch –¥–ª—è –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
# ============================================================

print("\n" + "=" * 70)
print("üìå –≠–¢–ê–ü 2: RandomizedSearch (–Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏)")
print("=" * 70)

for name, config in randomsearch_models.items():
    print(f"\n{'=' * 70}")
    print(f"üéØ {name} (RandomizedSearch)")
    print(f"{'=' * 70}")
    
    start = time.time()
    
    search = RandomizedSearchCV(
        estimator=config['model'],
        param_distributions=config['params'],
        n_iter=config['n_iter'],
        cv=3,
        scoring='f1',
        n_jobs=-1,
        verbose=0,
        random_state=42
    )
    
    search.fit(X_train, y_train)
    elapsed = time.time() - start
    
    y_pred = search.predict(X_test)
    y_proba = search.predict_proba(X_test)[:, 1] if hasattr(search, 'predict_proba') else None
    
    result = {
        'model': name,
        'search_type': 'RandomizedSearch',
        'best_params': search.best_params_,
        'cv_f1': search.best_score_,
        'test_f1': f1_score(y_test, y_pred),
        'test_accuracy': accuracy_score(y_test, y_pred),
        'test_roc_auc': roc_auc_score(y_test, y_proba) if y_proba is not None else None,
        'baseline_f1': None,
        'improvement': None,
        'time_min': elapsed / 60,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    best_models[name] = search
    all_results = save_results_incremental(result, name)
    
    print(f"‚è±Ô∏è –í—Ä–µ–º—è: {elapsed/60:.1f} –º–∏–Ω")
    print(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {search.best_params_}")
    print(f"üìä CV F1: {search.best_score_:.4f}")
    print(f"üìä Test F1: {f1_score(y_test, y_pred):.4f}")

# ============================================================
# 8. –§–ò–ù–ê–õ–¨–ù–´–ô –ê–ù–°–ê–ú–ë–õ–¨ (–¢–û–ü-5 –ú–û–î–ï–õ–ï–ô)
# ============================================================

print("\n" + "=" * 70)
print("üìå –≠–¢–ê–ü 3: –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å")
print("=" * 70)

results_sorted = sorted(all_results, key=lambda x: x['test_f1'], reverse=True)
top_5 = results_sorted[:5]

print("\nüèÜ –¢–æ–ø-5 –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è:")
estimators = []

for i, res in enumerate(top_5, 1):
    print(f"{i}. {res['model']} (F1={res['test_f1']:.4f}, {res['search_type']})")
    estimators.append((res['model'], best_models[res['model']].best_estimator_))

voting = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)
voting.fit(X_train, y_train)

y_pred_ensemble = voting.predict(X_test)
y_proba_ensemble = voting.predict_proba(X_test)[:, 1]

ensemble_result = {
    'model': 'VotingEnsemble_Final',
    'search_type': 'Ensemble',
    'test_f1': f1_score(y_test, y_pred_ensemble),
    'test_accuracy': accuracy_score(y_test, y_pred_ensemble),
    'test_roc_auc': roc_auc_score(y_test, y_proba_ensemble),
    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
}

all_results = save_results_incremental(ensemble_result, 'VotingEnsemble_Final')

print(f"\nüéØ –ê–ù–°–ê–ú–ë–õ–¨:")
print(f"   Accuracy: {ensemble_result['test_accuracy']:.4f}")
print(f"   F1-Score: {ensemble_result['test_f1']:.4f}")
print(f"   ROC-AUC:  {ensemble_result['test_roc_auc']:.4f}")

# ============================================================
# 9. –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–Å–¢
# ============================================================

total_elapsed = time.time() - total_start

print("\n" + "=" * 70)
print("üìä –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–Å–¢")
print("=" * 70)

print(f"\n{'#':<3} {'–ú–æ–¥–µ–ª—å':<20} {'–¢–∏–ø':<15} {'Test F1':<10} {'–£–ª—É—á—à–µ–Ω–∏–µ':<10} {'–í—Ä–µ–º—è':<10}")
print("-" * 70)

for i, res in enumerate(results_sorted, 1):
    imp_str = f"{res['improvement']:+.4f}" if res['improvement'] is not None else "N/A"
    print(f"{i:<3} {res['model']:<20} {res['search_type']:<15} {res['test_f1']:<10.4f} {imp_str:<10} {res['time_min']:<10.1f} –º–∏–Ω")

print(f"\n‚è±Ô∏è –û–ë–©–ï–ï –í–†–ï–ú–Ø: {total_elapsed/60:.1f} –º–∏–Ω—É—Ç ({total_elapsed/3600:.2f} —á–∞—Å–æ–≤)")

print("\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
print("   ‚Ä¢ hybrid_search_results.json (–≤—Å–µ –º–æ–¥–µ–ª–∏)")
print("   ‚Ä¢ model_result_*.json (–∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –æ—Ç–¥–µ–ª—å–Ω–æ)")

print(f"\nüïê –ö–æ–Ω–µ—Ü: {time.strftime('%H:%M:%S')}")
print("=" * 70)
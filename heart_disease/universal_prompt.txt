─────────────────────────────────────────────────────────────────────────
  ML MASTER PIPELINE — УНИВЕРСАЛЬНЫЙ ПРОМПТ ДЛЯ МАКСИМАЛЬНОГО F1/SCORE
─────────────────────────────────────────────────────────────────────────

Ты — эксперт по машинному обучению с опытом в 100+ Kaggle-соревнованиях.
Твоя задача: выжать максимум из данных за 2-3 часа, используя все 
известные стратегии оптимизации.

─────────────────────────────────────────────────────────────────────────
  ЭТАП 0: АНАЛИЗ ЗАДАЧИ (5 минут)
─────────────────────────────────────────────────────────────────────────

1. Определи тип задачи:
   □ Классификация (binary/multi-class)
   □ Регрессия
   □ Ранжирование
   □ Другое

2. Оцени данные:
   • Размер выборки: _______ строк
   • Количество признаков: _______
   • Баланс классов: _______ (если классификация)
   • Пропуски: _______%
   • Типы признаков: cat _______ | num _______ | text _______

3. Выбери целевую метрику:
   • Классификация: F1 / ROC-AUC / Accuracy / MCC
   • Регрессия: RMSE / MAE / R²
   • Другое: _______

─────────────────────────────────────────────────────────────────────────
  ЭТАП 1: БАЗОВЫЕ МОДЕЛИ (15 минут)
─────────────────────────────────────────────────────────────────────────

Обучи 5 моделей на дефолтных параметрах для бейзлайна:

1. LogisticRegression / LinearRegression
2. RandomForest
3. LightGBM
4. XGBoost
5. CatBoost (если классификация/регрессия)

✅ Сохрани результаты в таблицу:
┌─────────────┬─────────────┬─────────────┬─────────────┐
│ Модель      │ Метрика     │ Время       │ Статус      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│ LightGBM    │ _______     │ _______     │ ✅/❌       │
│ XGBoost     │ _______     │ _______     │ ✅/❌       │
│ CatBoost    │ _______     │ _______     │ ✅/❌       │
│ RandomForest│ _______     │ _______     │ ✅/❌       │
│ Linear      │ _______     │ _______     │ ✅/❌       │
└─────────────┴─────────────┴─────────────┴─────────────┘

✅ Выбери топ-3 модели для дальнейшей оптимизации.

─────────────────────────────────────────────────────────────────────────
  ЭТАП 2: FEATURE ENGINEERING (30 минут)
─────────────────────────────────────────────────────────────────────────

Создай новые признаки (применимо к данным):

□ Взаимодействия: feat1 * feat2, feat1 / feat2
□ Полиномиальные: feat², feat³
□ Агрегации: mean, std, min, max по группам
□ Encoding: Target Encoding, Frequency Encoding (для cat)
□ Scaling: StandardScaler, RobustScaler (для linear/SVM)
□ Outliers: clipping, log-transform

✅ Оцени важность признаков (feature_importance / permutation)
✅ Удали признаки с importance < threshold (например, 0.01)
✅ Проверь корреляции (>0.95 — удали один)

─────────────────────────────────────────────────────────────────────────
  ЭТАП 3: COARSE-TO-FINE GRIDSEARCH (60 минут)
─────────────────────────────────────────────────────────────────────────

Для топ-3 моделей примени 2-3 прохода GridSearch:

┌───────────────────────────────────────────────────────────────────────┐
│  ПРОХОД 1: COARSE (широкий диапазон, большой шаг)                    │
│  • n_estimators: [50, 100, 200]                                      │
│  • max_depth: [3, 6, 10, 15]                                         │
│  • learning_rate: [0.01, 0.1, 0.2]                                   │
│  • CV: 3 фолда (быстро)                                              │
│  • Выборка: 50-100k строк (если >100k)                               │
└───────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────┐
│  ПРОХОД 2: FINE (узкий диапазон вокруг лучших, малый шаг)            │
│  • n_estimators: [best±50]                                           │
│  • max_depth: [best±2]                                               │
│  • learning_rate: [best±0.05]                                        │
│  • CV: 5 фолдов                                                      │
│  • Выборка: полная или 150k                                          │
└───────────────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────────────┐
│  ПРОХОД 3: ULTRA-FINE (опционально, если время есть)                 │
│  • Только 1-2 критичных параметра                                    │
│  • Шаг: минимальный (например, lr: [0.09, 0.10, 0.11])               │
│  • CV: 5 фолдов                                                      │
└───────────────────────────────────────────────────────────────────────┘

✅ Альтернатива GridSearch: Optuna (100 триалов, 30 минут)
✅ Сохраняй лучшую модель после каждого прохода (joblib)

─────────────────────────────────────────────────────────────────────────
  ЭТАП 4: THRESHOLD TUNING (10 минут, для классификации)
─────────────────────────────────────────────────────────────────────────

Для каждой модели найди оптимальный порог:

```python
for threshold in np.arange(0.30, 0.70, 0.01):
    y_pred = (y_proba >= threshold).astype(int)
    f1 = f1_score(y_test, y_pred)
    # сохрани лучший порог